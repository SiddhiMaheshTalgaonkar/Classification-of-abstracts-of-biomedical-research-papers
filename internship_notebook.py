# -*- coding: utf-8 -*-
"""Internship notebook

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/123ifezjB_754_ON9_-dxJKf2Akm_OrP2
"""

# Install the required library 'stanza'
!pip install stanza

# Import necessary libraries
import pandas as pd
import stanza

# Load the Stanza pipeline for English
nlp = stanza.Pipeline('en')

# Load the Excel file containing the data
data = pd.read_excel('/content/Book1.xlsx')

# Define a function to perform POS tagging and count the number of each POS tag
def count_pos_tags(text):
    # Perform POS tagging on the input text
    doc = nlp(text)

    # Initialize counters for each POS tag
    num_nouns = 0
    num_pronouns = 0
    num_verbs = 0
    num_adverbs = 0
    num_adjectives = 0

    # Count the number of each POS tag in the tagged text
    for sent in doc.sentences:
        for word in sent.words:
            if word.pos == 'NOUN':
                num_nouns += 1
            elif word.pos == 'PRON':
                num_pronouns += 1
            elif word.pos == 'VERB':
                num_verbs += 1
            elif word.pos == 'ADV':
                num_adverbs += 1
            elif word.pos == 'ADJ':
                num_adjectives += 1

    # Return the counts as a dictionary
    return {'num_nouns': num_nouns, 'num_pronouns': num_pronouns, 'num_verbs': num_verbs, 'num_adverbs': num_adverbs, 'num_adjectives': num_adjectives}

# Apply the count_pos_tags function to each abstract in the Excel file
# and add the POS tag counts as new columns to the DataFrame
data[['num_nouns', 'num_pronouns', 'num_verbs', 'num_adverbs', 'num_adjectives']] = data['Abstract'].apply(lambda x: pd.Series(list(count_pos_tags(x).values()) if isinstance(x, str) else [0, 0, 0, 0, 0]))

# Save the updated DataFrame to a new Excel file
data.to_excel('/content/Book12.xlsx', index=False)

# Install the required library 'stanza'
!pip install stanza

# Import necessary libraries
import openpyxl
import stanza
import matplotlib.pyplot as plt
import nltk
import pandas as pd
import numpy as np

# Download the 'punkt' resource from NLTK
nltk.download('punkt')

# Download the English language model for Stanza
stanza.download('en')

# Read the data from an Excel file named 'File.xlsx'
data = pd.read_excel('/content/File.xlsx')

# Calculate the POS tag frequencies for each abstract
# Tokenize each abstract using NLTK's word_tokenize function and then apply POS tagging using NLTK's pos_tag function
# Count the number of adjectives, adverbs, and pronouns in each abstract and store the results in separate columns
data['POS'] = data['Abstract'].apply(nltk.word_tokenize).apply(nltk.pos_tag)
data['Adjectives'] = data['POS'].apply(lambda x: len([w for w, pos in x if pos.startswith('JJ')]))
data['Adverbs'] = data['POS'].apply(lambda x: len([w for w, pos in x if pos.startswith('RB')]))
data['Pronouns'] = data['POS'].apply(lambda x: len([w for w, pos in x if pos.startswith('PR')]))

# Calculate the ratios for each abstract
# Calculate the ratio of adverbs to adjectives (R1) and the ratio of adjectives to pronouns (R2)
data['R1'] = data['Adverbs'] / data['Adjectives']
data['R2'] = data['Adjectives'] / data['Pronouns']

# Get unique categories from the 'Category' column
categories = data['Category'].unique()

# Create a color map for categories using the 'tab10' colormap from Matplotlib
color_map = plt.cm.get_cmap('tab10', len(categories))

# Plot the data for each category with different colors
fig, ax = plt.subplots()
for i, category in enumerate(categories):
    # Get data for the current category
    category_data = data[data['Category'] == category]
    # Scatter plot for the current category with color from the colormap and add a label for the legend
    ax.scatter(category_data['R1'], category_data['R2'], color=color_map(i), label=category, alpha=0.7)

# Set labels and title for the plot
plt.xlabel('Ratio of Adverbs to Adjectives (R1)')
plt.ylabel('Ratio of Adjectives to Pronouns (R2)')
plt.title('POS Tag Ratios by Category')
plt.legend()  # Show the legend
plt.show()  # Display the plot

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Read the data from the Excel sheet named 'File.xlsx'
data = pd.read_excel('/content/File.xlsx')

# Select the columns with POS tag frequencies from the DataFrame
pos_columns = ['num_nouns', 'num_pronouns', 'num_verbs', 'num_adverbs', 'num_adjectives']

# Group the data by 'Category' and calculate descriptive statistics for each group
grouped_stats = data.groupby('Category')[pos_columns].describe()

# Create box plots for each POS tag frequency column by category
for column in pos_columns:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x='Category', y=column, data=data)
    plt.xlabel('Category')
    plt.ylabel('Frequency')
    plt.title(f'{column.capitalize()} Frequency Distribution by Category')
    plt.xticks(rotation=90)
    plt.show()

# Create line graphs for POS tag frequency columns
for column in pos_columns:
    plt.figure(figsize=(10, 6))
    for category in data['Category'].unique():
        # Get data for the current category
        category_data = data[data['Category'] == category]
        # Plot line graph for the current category
        plt.plot(category_data.index, category_data[column], label=category)
    plt.xlabel('Index')
    plt.ylabel('Frequency')
    plt.title(f'{column.capitalize()} Frequency Over Index')
    plt.legend()
    plt.show()

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.naive_bayes import GaussianNB

# Read the data from the Excel file named 'File.xlsx'
df = pd.read_excel('/content/File.xlsx')

# Get unique categories from the 'Category' column
categories = df['Category'].unique()

# Define the features for histogram plotting
features = ['num_nouns', 'num_pronouns', 'num_verbs', 'num_adverbs', 'num_adjectives']

# Loop over each category
for category in categories:
    # Select data for the current category
    category_data = df[df['Category'] == category]

    # Plot histograms for each feature in the current category
    for feature in features:
        # Get counts for the current feature
        counts = category_data[feature]
        # Calculate the probability of each count and sort them in ascending order
        counts_prob = counts.value_counts(normalize=True).sort_index()

        # Plot the histogram for the current feature
        plt.figure(figsize=(8, 6))
        plt.bar(counts_prob.index, counts_prob.values, alpha=0.5, color='purple', edgecolor='black')
        plt.title(f'Histogram of {feature} - Category: {category}')
        plt.xlabel('Counts')
        plt.ylabel('Probability')
        plt.show()

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Step 1: Prepare the data
# Read the data from the Excel file named 'File.xlsx'
data = pd.read_excel('/content/File.xlsx')
abstracts = data['Abstract']  # Extract the 'Abstract' column as features
categories = data['Category']  # Extract the 'Category' column as target labels

# Step 2: Extract features
# Use CountVectorizer to convert abstracts into feature vectors (word counts)
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(abstracts)
features = pd.DataFrame(X.toarray())

# Calculate the ratio of adverbs and adjectives in each abstract
num_adverbs = features.sum(axis=1)
num_adjectives = features.sum(axis=1)
total_words = features.sum(axis=1)
features['adverb_ratio'] = num_adverbs / total_words
features['adjective_ratio'] = num_adjectives / total_words

# Calculate the ratio of adjectives and pronouns in each abstract
num_pronouns = features.sum(axis=1)
features['pronoun_ratio'] = num_pronouns / num_adjectives

# Step 3: Split the data into training and testing sets
# Use train_test_split to split the data into 80% training and 20% testing sets
X_train, X_test, y_train, y_test = train_test_split(features, categories, test_size=0.2, random_state=42)

# Convert feature names to strings (to avoid potential issues)
X_train.columns = X_train.columns.astype(str)
X_test.columns = X_test.columns.astype(str)

# Reset indices of X_test, y_test, and y_pred
X_test.reset_index(drop=True, inplace=True)
y_test.reset_index(drop=True, inplace=True)

# Step 4: Train the Naive Bayes model
# Use MultinomialNB as the Naive Bayes classifier
model = MultinomialNB()
model.fit(X_train, y_train)

# Step 5: Make predictions on the test data
y_pred = model.predict(X_test)

# Step 6: Evaluate the model
# Calculate the overall accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Overall Accuracy:", accuracy)

# Calculate accuracy for each category
category_accuracy = {}
unique_categories = categories.unique()
for category in unique_categories:
    category_indices = y_test[y_test == category].index
    category_y_test = y_test.loc[category_indices]
    category_y_pred = y_pred[category_indices]
    category_accuracy[category] = accuracy_score(category_y_test, category_y_pred)
    print(f"Accuracy for {category}: {category_accuracy[category]}")

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate other metrics: precision, recall, f1-score
classification_rep = classification_report(y_test, y_pred)

# Print the confusion matrix and classification report
print("Confusion Matrix:")
print(cm)
print("\nClassification Report:")
print(classification_rep)

# Create a new DataFrame with the metrics
metrics_df = pd.DataFrame({
    'Category': unique_categories,
    'Accuracy': [category_accuracy[category] for category in unique_categories],
    'True Positive': [cm[i, i] for i in range(len(unique_categories))],
    'False Positive': [sum(cm[:, i]) - cm[i, i] for i in range(len(unique_categories))],
    'False Negative': [sum(cm[i, :]) - cm[i, i] for i in range(len(unique_categories))]
})

# Calculate precision, recall, and f1-score for each category
metrics_df['Precision'] = metrics_df['True Positive'] / (metrics_df['True Positive'] + metrics_df['False Positive'])
metrics_df['Recall'] = metrics_df['True Positive'] / (metrics_df['True Positive'] + metrics_df['False Negative'])
metrics_df['F1-Score'] = 2 * (metrics_df['Precision'] * metrics_df['Recall']) / (metrics_df['Precision'] + metrics_df['Recall'])

# Save the metrics to a new Excel file named 'Metrics.xlsx'
metrics_df.to_excel('/content/Metrics.xlsx', index=False)

# Plot the graph of accuracy by category
plt.figure(figsize=(8, 6))
plt.bar(metrics_df['Category'], metrics_df['Accuracy'])
plt.xlabel("Category")
plt.ylabel("Accuracy")
plt.title("Accuracy by Category")
plt.xticks(rotation=45)
plt.show()

# Import necessary libraries
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

# Step 1: Read the data from the Excel file named 'File.xlsx'
data = pd.read_excel('/content/File.xlsx')
abstracts = data['Abstract']  # Extract the 'Abstract' column as features
categories = data['Category']  # Extract the 'Category' column as target labels

# Step 2: Initialize a CountVectorizer to convert abstracts into feature vectors
vectorizer = CountVectorizer()

# Step 3: Transform the abstracts into feature vectors (word frequency counts)
X = vectorizer.fit_transform(abstracts)

# Step 4: Train the Naive Bayes model using MultinomialNB
model = MultinomialNB()
model.fit(X, categories)

# Step 5: Get the feature names (words) from the CountVectorizer
feature_names = vectorizer.get_feature_names_out()

# Step 6: Compare feature importances for pairs of categories using Naive Bayes model
unique_categories = categories.unique()
pairwise_important_words = {}
for i in range(len(unique_categories) - 1):
    for j in range(i + 1, len(unique_categories)):
        category1 = unique_categories[i]
        category2 = unique_categories[j]

        # Step 7: Get the feature importances (log probabilities) for each category
        category1_idx = model.classes_.tolist().index(category1)
        category2_idx = model.classes_.tolist().index(category2)
        category1_importance = model.feature_log_prob_[category1_idx]
        category2_importance = model.feature_log_prob_[category2_idx]

        # Step 8: Calculate the relative importance of words between the two categories
        relative_importance = category1_importance - category2_importance

        # Step 9: Get the top 10 relatively important words for each category
        top_10_words_idx = relative_importance.argsort()[-10:][::-1]
        important_words = [feature_names[idx] for idx in top_10_words_idx]

        # Step 10: Store the important words for the pair of categories
        pair_key = f"{category1} vs {category2}"
        pairwise_important_words[pair_key] = important_words

# Step 11: Print the relatively important words for each pair of categories
for pair, words in pairwise_important_words.items():
    print(f"Pair: {pair}")
    print("Relatively Important Words:", ", ".join(words))
    print()